{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 347988 samples\n"
     ]
    }
   ],
   "source": [
    "from llvm_ml.data import load_pyg_dataset\n",
    "# banned_ids = [\"clang_347660\", \"x264_29245\", \"clang_777523\", \"clang_1563767\", \"clang_1201822\", \"clang_2304591\", \"clang_307223\", \"clang_2536327\", \"clang_600001\", \"clang_930652\", \"x264_173431\", \"clang_61216\", \"clang_2288557\", \"clang_2287086\", \"x264_297519\", \"clang_1128560\", \"clang_2088021\", \"clang_467960\", \"clang_2300269\", \"clang_24730\", \"clang_2617009\", \"clang_770736\"]\n",
    "banned_ids = []\n",
    "# banned_ids = [\"x264_297519\", \"x264_310679\", \"x264_224705\", \"x264_227241\", \"x264_221993\"]\n",
    "dataset = load_pyg_dataset(\"./data/ryzen3600_v16.cbuf\", use_binary_opcode=False, banned_ids=banned_ids)\n",
    "print(f\"Training with {len(dataset)} samples\")\n",
    "# dataset.print_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T13:44:19.990522133Z",
     "start_time": "2023-09-10T13:41:12.532540133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch_geometric.loader import DataLoader\n",
    "from llvm_ml.utils import plot_histogram\n",
    "import torch.utils.data\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from model.estimation import GNNEstimation, LSTMEstimation\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from model.utils import plot_lift_chart\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
    "\n",
    "class GNN(pl.LightningModule):\n",
    "    def __init__(self, conv, input_dim, hidden_dim, output_dim, batch_size, learning_rate=0.005, debug=False):\n",
    "        super(GNN, self).__init__()\n",
    "        if conv == 'None':\n",
    "            self.model = LSTMEstimation(input_dim, hidden_dim, output_dim, batch_size)\n",
    "        else:\n",
    "            self.model = GNNEstimation(conv, input_dim, hidden_dim, output_dim, batch_size)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "        self.val_measurements = [[], []]\n",
    "\n",
    "        # Metrics\n",
    "        self.train_mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.val_mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.train_cosinesim = torchmetrics.CosineSimilarity()\n",
    "        self.val_cosinesim = torchmetrics.CosineSimilarity()\n",
    "        self.train_evar = torchmetrics.ExplainedVariance()\n",
    "        self.val_evar = torchmetrics.ExplainedVariance()\n",
    "        \n",
    "        self.loss_debug_data = []\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        bb, raw = batch\n",
    "        y_hat = self(bb)\n",
    "        loss = F.mse_loss(y_hat, bb.y)\n",
    "\n",
    "        # L1 regularization\n",
    "        # l1_lambda = 1e-5  # Regularization coefficient\n",
    "        # l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
    "        # loss = loss + l1_lambda * l1_norm\n",
    "        \n",
    "        self.train_mae(y_hat, bb.y)\n",
    "        self.train_cosinesim(y_hat, bb.y)\n",
    "        self.train_evar(y_hat, bb.y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.log(\"train_mae\", self.train_mae, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.log(\"train_cosine_similarity\", self.train_cosinesim, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.log(\"train_explained_variance\", self.train_evar, on_epoch=True, batch_size=self.batch_size)\n",
    "        \n",
    "        # if self.global_step == 0:\n",
    "        #     self.logger.experiment.add_graph(self, bb)\n",
    "\n",
    "        if self.debug and self.current_epoch == 15:\n",
    "            piece = {'loss': loss, 'data': []}\n",
    "            for i in range(self.batch_size):\n",
    "                piece['data'].append({\n",
    "                    'predicted': y_hat[i],\n",
    "                    'expected': bb.y[i],\n",
    "                    'source': raw['source'][i],\n",
    "                    'id': raw['id'][i]\n",
    "                })\n",
    "            \n",
    "            self.loss_debug_data.append(piece)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        bb, raw = batch\n",
    "        y_hat = self(bb)\n",
    "        loss = F.mse_loss(y_hat, bb.y)\n",
    "        self.val_mae(y_hat, bb.y)\n",
    "        self.val_cosinesim(y_hat, bb.y)\n",
    "        self.val_evar(y_hat, bb.y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.log(\"val_mae\", self.val_mae, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.log(\"val_cosine_similarity\", self.val_cosinesim, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.log(\"val_explained_variance\", self.val_evar, on_epoch=True, batch_size=self.batch_size)\n",
    "\n",
    "        for a, b in zip(y_hat, bb.y):\n",
    "            self.val_measurements[1].append(a.item())\n",
    "            self.val_measurements[0].append(b.item())\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            num_samples_to_log = min(self.batch_size, 5)\n",
    "            for i in range(num_samples_to_log):\n",
    "                self.logger.experiment.add_scalar(f\"val/sample_{i}/true\", bb.y[i].item(), self.current_epoch)\n",
    "                self.logger.experiment.add_scalar(f\"val/sample_{i}/predicted\", y_hat[i].item(), self.current_epoch)\n",
    "                if self.global_step == 0:\n",
    "                    self.logger.experiment.add_text(f\"val/sample_{i}/source\", raw['source'][i], self.global_step)\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        if self.debug and self.current_epoch == 15:\n",
    "            max_sample = max(self.loss_debug_data, key=lambda d: d['loss'])\n",
    "            print(f\"Max loss is {max_sample['loss']}\")\n",
    "            \n",
    "            for bb in max_sample['data']:\n",
    "                print(f\"Predicted {bb['predicted']}, expected: {bb['expected']}, id: {bb['id']}\\nsource:\\n{bb['source']}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.debug:\n",
    "            # x = np.asarray(self.val_measurements[0])\n",
    "            # y = np.asarray(self.val_measurements[1])\n",
    "            # if np.any(x) and np.any(y):\n",
    "            #    plot = plot_histogram(x, y, percentile=0.95)\n",
    "            #    image = PIL.Image.open(plot)\n",
    "            #    image = ToTensor()(image).unsqueeze(0)\n",
    "            #    self.logger.experiment.add_image(\"val_histogram\", image[0], self.current_epoch)\n",
    "                \n",
    "            lift_chart = plot_lift_chart(self.val_measurements)\n",
    "            image = PIL.Image.open(lift_chart)\n",
    "            image = ToTensor()(image).unsqueeze(0)\n",
    "            self.logger.experiment.add_image(\"val_lift_chart\", image[0], self.current_epoch)\n",
    "        self.val_measurements = [[], []]\n",
    "        \n",
    "    def on_train_start(self) -> None:\n",
    "        self.logger.log_graph(self)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True, min_lr=1e-6, cooldown=5)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 7, 10, 15, 25, 30], gamma=0.1, verbose=False)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T13:44:20.042854362Z",
     "start_time": "2023-09-10T13:44:19.999188401Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | model           | LSTMEstimation    | 6.6 M \n",
      "1 | train_mae       | MeanAbsoluteError | 0     \n",
      "2 | val_mae         | MeanAbsoluteError | 0     \n",
      "3 | train_cosinesim | CosineSimilarity  | 0     \n",
      "4 | val_cosinesim   | CosineSimilarity  | 0     \n",
      "5 | train_evar      | ExplainedVariance | 0     \n",
      "6 | val_evar        | ExplainedVariance | 0     \n",
      "------------------------------------------------------\n",
      "6.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.6 M     Total params\n",
      "26.245    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42d07a85f3214d5eb0abd059a563f5b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9350bcf823fb4df5bf27286159ef6f7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1519dd133af4a74b0dd210b392c89f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b2d2df513204dc99dfdb674a6cee12b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3f5daa4bf254872932ae380aa5b7b51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e18b3d736df54aa7a63963aad463f815"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa7287eca2e84bfcb7cc984e36e57cb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a4eb19332ba4c44ba4c9ee30d89b540"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c813f2014f048049aa8d56c9fcb5bb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "076cb85d7b814db6a8dbd545de2fdefd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3dd36c8d1704d5b829cf149a1f0762d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "632429a2faac4de7ad43e88744fcc6a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f03d1c14f6e944679a891f6ad9fea677"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "581fa235a02848f5bddc36f40becdb51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1c97a0b787746deb7ac43019ba9a6b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b159e942748f4052b65a0ff058394ce0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad173728bf0947d89811b1048ad8ca82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd0cad18989344cd876df21879a85175"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d5dd0f4a8934505a2edac1ff4f04dba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80d96d4388994bb494452a15e26abdd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5651844778c240dc88400c9d2488e1bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbd904cd1f9942acb4e2aba1800365f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "882ef4fb8fce4d1d91b828b4b568d564"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/cpu-uarch-prediction/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "num_training = int(0.7 * len(dataset))\n",
    "num_val = len(dataset) - num_training\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [num_training, num_val])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "\n",
    "model = GNN(\"None\", 256, hidden_dim, output_dim, batch_size, debug=False)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./data/mcmlm.chkpt\"), strict=False)\n",
    "\n",
    "logger = TensorBoardLogger(\"runs\", name=\"3600_estimate_embedding\")\n",
    "logger.log_graph(model)\n",
    "trainer = pl.Trainer(max_epochs=40, logger=logger)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T13:56:38.396999719Z",
     "start_time": "2023-09-10T13:44:20.012589302Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
