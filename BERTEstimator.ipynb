{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T12:59:26.707218227Z",
     "start_time": "2023-11-05T12:58:46.650442266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 347988 samples\n"
     ]
    }
   ],
   "source": [
    "from llvm_ml.torch import BasicBlockDataset\n",
    "banned_ids = []\n",
    "dataset = BasicBlockDataset(\"./data/ryzen3600_v16.cbuf\", masked=False, banned_ids=banned_ids, prefilter=True)\n",
    "print(f\"Training with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                             | Type               | Params\n",
      "-------------------------------------------------------------------------\n",
      "0  | embedding                        | MCEmbedding        | 1.3 M \n",
      "1  | embedding.embedding              | Embedding          | 1.3 M \n",
      "2  | embedding.pos_encoding           | PositionalEncoding | 0     \n",
      "3  | embedding.norm                   | LayerNorm          | 128   \n",
      "4  | encoders                         | ModuleList         | 298 K \n",
      "5  | encoders.0                       | MCGraphEncoder     | 49.7 K\n",
      "6  | encoders.0.norm1                 | LayerNorm          | 128   \n",
      "7  | encoders.0.attention             | MCGraphAttention   | 16.4 K\n",
      "8  | encoders.0.attention.heads       | MCAttentionHead    | 16.4 K\n",
      "9  | encoders.0.attention.heads.key   | Linear             | 4.1 K \n",
      "10 | encoders.0.attention.heads.query | Linear             | 4.1 K \n",
      "11 | encoders.0.attention.heads.value | Linear             | 4.1 K \n",
      "12 | encoders.0.attention.heads.proj  | Linear             | 4.1 K \n",
      "13 | encoders.0.norm2                 | LayerNorm          | 128   \n",
      "14 | encoders.0.feed_forward          | Sequential         | 33.1 K\n",
      "15 | encoders.0.feed_forward.0        | Linear             | 16.6 K\n",
      "16 | encoders.0.feed_forward.1        | GELU               | 0     \n",
      "17 | encoders.0.feed_forward.2        | Linear             | 16.4 K\n",
      "18 | encoders.0.feed_forward.3        | Dropout            | 0     \n",
      "19 | encoders.1                       | MCGraphEncoder     | 49.7 K\n",
      "20 | encoders.1.norm1                 | LayerNorm          | 128   \n",
      "21 | encoders.1.attention             | MCGraphAttention   | 16.4 K\n",
      "22 | encoders.1.attention.heads       | MCAttentionHead    | 16.4 K\n",
      "23 | encoders.1.attention.heads.key   | Linear             | 4.1 K \n",
      "24 | encoders.1.attention.heads.query | Linear             | 4.1 K \n",
      "25 | encoders.1.attention.heads.value | Linear             | 4.1 K \n",
      "26 | encoders.1.attention.heads.proj  | Linear             | 4.1 K \n",
      "27 | encoders.1.norm2                 | LayerNorm          | 128   \n",
      "28 | encoders.1.feed_forward          | Sequential         | 33.1 K\n",
      "29 | encoders.1.feed_forward.0        | Linear             | 16.6 K\n",
      "30 | encoders.1.feed_forward.1        | GELU               | 0     \n",
      "31 | encoders.1.feed_forward.2        | Linear             | 16.4 K\n",
      "32 | encoders.1.feed_forward.3        | Dropout            | 0     \n",
      "33 | encoders.2                       | MCGraphEncoder     | 49.7 K\n",
      "34 | encoders.2.norm1                 | LayerNorm          | 128   \n",
      "35 | encoders.2.attention             | MCGraphAttention   | 16.4 K\n",
      "36 | encoders.2.attention.heads       | MCAttentionHead    | 16.4 K\n",
      "37 | encoders.2.attention.heads.key   | Linear             | 4.1 K \n",
      "38 | encoders.2.attention.heads.query | Linear             | 4.1 K \n",
      "39 | encoders.2.attention.heads.value | Linear             | 4.1 K \n",
      "40 | encoders.2.attention.heads.proj  | Linear             | 4.1 K \n",
      "41 | encoders.2.norm2                 | LayerNorm          | 128   \n",
      "42 | encoders.2.feed_forward          | Sequential         | 33.1 K\n",
      "43 | encoders.2.feed_forward.0        | Linear             | 16.6 K\n",
      "44 | encoders.2.feed_forward.1        | GELU               | 0     \n",
      "45 | encoders.2.feed_forward.2        | Linear             | 16.4 K\n",
      "46 | encoders.2.feed_forward.3        | Dropout            | 0     \n",
      "47 | encoders.3                       | MCGraphEncoder     | 49.7 K\n",
      "48 | encoders.3.norm1                 | LayerNorm          | 128   \n",
      "49 | encoders.3.attention             | MCGraphAttention   | 16.4 K\n",
      "50 | encoders.3.attention.heads       | MCAttentionHead    | 16.4 K\n",
      "51 | encoders.3.attention.heads.key   | Linear             | 4.1 K \n",
      "52 | encoders.3.attention.heads.query | Linear             | 4.1 K \n",
      "53 | encoders.3.attention.heads.value | Linear             | 4.1 K \n",
      "54 | encoders.3.attention.heads.proj  | Linear             | 4.1 K \n",
      "55 | encoders.3.norm2                 | LayerNorm          | 128   \n",
      "56 | encoders.3.feed_forward          | Sequential         | 33.1 K\n",
      "57 | encoders.3.feed_forward.0        | Linear             | 16.6 K\n",
      "58 | encoders.3.feed_forward.1        | GELU               | 0     \n",
      "59 | encoders.3.feed_forward.2        | Linear             | 16.4 K\n",
      "60 | encoders.3.feed_forward.3        | Dropout            | 0     \n",
      "61 | encoders.4                       | MCGraphEncoder     | 49.7 K\n",
      "62 | encoders.4.norm1                 | LayerNorm          | 128   \n",
      "63 | encoders.4.attention             | MCGraphAttention   | 16.4 K\n",
      "64 | encoders.4.attention.heads       | MCAttentionHead    | 16.4 K\n",
      "65 | encoders.4.attention.heads.key   | Linear             | 4.1 K \n",
      "66 | encoders.4.attention.heads.query | Linear             | 4.1 K \n",
      "67 | encoders.4.attention.heads.value | Linear             | 4.1 K \n",
      "68 | encoders.4.attention.heads.proj  | Linear             | 4.1 K \n",
      "69 | encoders.4.norm2                 | LayerNorm          | 128   \n",
      "70 | encoders.4.feed_forward          | Sequential         | 33.1 K\n",
      "71 | encoders.4.feed_forward.0        | Linear             | 16.6 K\n",
      "72 | encoders.4.feed_forward.1        | GELU               | 0     \n",
      "73 | encoders.4.feed_forward.2        | Linear             | 16.4 K\n",
      "74 | encoders.4.feed_forward.3        | Dropout            | 0     \n",
      "75 | encoders.5                       | MCGraphEncoder     | 49.7 K\n",
      "76 | encoders.5.norm1                 | LayerNorm          | 128   \n",
      "77 | encoders.5.attention             | MCGraphAttention   | 16.4 K\n",
      "78 | encoders.5.attention.heads       | MCAttentionHead    | 16.4 K\n",
      "79 | encoders.5.attention.heads.key   | Linear             | 4.1 K \n",
      "80 | encoders.5.attention.heads.query | Linear             | 4.1 K \n",
      "81 | encoders.5.attention.heads.value | Linear             | 4.1 K \n",
      "82 | encoders.5.attention.heads.proj  | Linear             | 4.1 K \n",
      "83 | encoders.5.norm2                 | LayerNorm          | 128   \n",
      "84 | encoders.5.feed_forward          | Sequential         | 33.1 K\n",
      "85 | encoders.5.feed_forward.0        | Linear             | 16.6 K\n",
      "86 | encoders.5.feed_forward.1        | GELU               | 0     \n",
      "87 | encoders.5.feed_forward.2        | Linear             | 16.4 K\n",
      "88 | encoders.5.feed_forward.3        | Dropout            | 0     \n",
      "89 | token_prediction                 | Linear             | 1.4 M \n",
      "90 | proj                             | Sequential         | 33.1 K\n",
      "91 | proj.0                           | Linear             | 16.6 K\n",
      "92 | proj.1                           | Linear             | 16.4 K\n",
      "93 | regression                       | Sequential         | 65    \n",
      "94 | regression.0                     | Linear             | 65    \n",
      "95 | train_mae                        | MeanAbsoluteError  | 0     \n",
      "96 | val_mae                          | MeanAbsoluteError  | 0     \n",
      "-------------------------------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "12.164    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e01b6e41ac7c49109fb575ca867e64a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21d628061a7744f2afc90cb37fb5d563"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38354ce2b47d495bb037eb9e76c84b04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28d682c0986f42cbadf52607a173fee0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b614be7edde947bca9c2c8304f1046c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6de38eda157f41d5b7a6df778cbdcd58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c2c76f076d6467cae45180961fcf5ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71df66613c9a47689456660727687891"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b9de9af18e54772a0a0d800246084e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f576ef86e1944bf69742e1099bde3e12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "877da07f5ee348598dd6f9268bacf557"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d211dfb0f98c4add9ce0b09f83aecd7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7797abc7906e410ca44f51721966ddaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d931e0d5d7244323bb15a5529de7f159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "127d352c24e14f3da9d965e74f52cf39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a49dd2253f247698175c30306256107"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c9077ed11a84d648b932e9e61e3ccd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4670cd341676406da1c6abf606c8cd73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "759cb3e4fb4141a3bbc801a774024949"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4037184abbf4d48aae99ceee3a47d68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d23be4352ec4f38afe87b97c1cf80ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4c3b1a840424382bbcc6ccdca9e74b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb633f1b8d654974bbf2639b403c065c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7491f22fcde04c52ba7aef49cfb2aa0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6df6e23c8017463e9ea266c1b093ae1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f607e482f304b1897ebfbe5dd2a1dc0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "199ad75c7a704e1499600f0db1d1693d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "from model.GraphBERT import ThroughputEstimator\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "from model import GraphBERT\n",
    "from lightning.pytorch.callbacks import ModelSummary, LearningRateMonitor\n",
    "\n",
    "import importlib\n",
    "importlib.reload(GraphBERT)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
    "\n",
    "batch_size = 256\n",
    "hidden_dim = 128\n",
    "emb_size = 64\n",
    "\n",
    "num_training = int(0.7 * len(dataset))\n",
    "num_val = len(dataset) - num_training\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [num_training, num_val])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "\n",
    "config = {\n",
    "    'num_opcodes': dataset.num_opcodes,\n",
    "    'embedding_size': emb_size,\n",
    "    'batch_size': batch_size,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'learning_rate': 3e-4,\n",
    "    'mode': 'pretrain',\n",
    "    #'learning_rate': 2e-5,\n",
    "    #'mode': 'regression',\n",
    "    'num_encoders': 6,\n",
    "    'num_head_encoder': 8\n",
    "}\n",
    "\n",
    "model = ThroughputEstimator(config)\n",
    "# model.load_state_dict(torch.load(\"data/pretrained_mcmlm.ckpt\")['state_dict'])\n",
    "\n",
    "logger = TensorBoardLogger(\"runs\", name=\"bert\")\n",
    "logger.log_graph(model)\n",
    "callbacks = [\n",
    "    ModelSummary(max_depth=-1),\n",
    "    LearningRateMonitor(),\n",
    "]\n",
    "trainer = pl.Trainer(max_epochs=25,\n",
    "                     logger=logger,\n",
    "                     precision='16-mixed',\n",
    "                     callbacks=callbacks,\n",
    "                     # fast_dev_run=True,\n",
    "                     #overfit_batches=1,\n",
    "                     #log_every_n_steps=1,\n",
    "                     )\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T11:33:42.105775849Z",
     "start_time": "2023-09-30T10:41:18.559331985Z"
    }
   },
   "id": "3ed1dc75f632603"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce507258e5bce8d7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, \"data/pretrained_mcmlm.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T10:34:24.757338053Z",
     "start_time": "2023-09-30T10:34:24.715909493Z"
    }
   },
   "id": "f1a90dfece6626c4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c059ecef510633c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save({'state_dict': model.state_dict()}, \"data/ryzen3600.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-30T09:08:26.249165838Z"
    }
   },
   "id": "8ddcfa81c7e3cfea"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "951e8d64644674f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
