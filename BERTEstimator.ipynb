{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "from torch.nn import Module\n",
    "from llvm_ml.torch.nn import MCNNConfig\n",
    "from lightning import pytorch as pl\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "class MCEmbedding(Module):\n",
    "    def __init__(self, num_opcodes, emb_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_opcodes, emb_size)\n",
    "        self.gcn_embedding = nn.Embedding(num_opcodes, emb_size)\n",
    "        self.pos_encoding = gnn.PositionalEncoding(emb_size)\n",
    "        self.gcn = gnn.GCNConv(emb_size, emb_size)\n",
    "        self.norm = gnn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, input_tensor, edge_index):\n",
    "        pos_tensor = self.pos_encoding(input_tensor)\n",
    "\n",
    "        gcn_tensor = self.gcn(self.gcn_embedding(input_tensor), edge_index)\n",
    "\n",
    "        output = self.embedding(input_tensor) + pos_tensor + gcn_tensor\n",
    "\n",
    "        return self.norm(output), pos_tensor, gcn_tensor\n",
    "\n",
    "\n",
    "class MCBERT(pl.LightningModule):\n",
    "    def __init__(self, config: MCNNConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = MCEmbedding(self.config.num_opcodes, self.config.embedding_size)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.config.embedding_size, nhead=self.config.num_heads_encoder, dropout=self.config.dropout, activation=\"relu\", batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.config.num_encoders)\n",
    "        \n",
    "        self.token_prediction = nn.Linear(self.config.embedding_size, self.config.num_opcodes)\n",
    "\n",
    "\n",
    "    def forward(self, nodes, edge_index, batch):\n",
    "        embedded, _, _ = self.embedding(nodes, edge_index)\n",
    "\n",
    "        encoded, mask = to_dense_batch(embedded, batch)\n",
    "\n",
    "        # TODO this does not do what I expected\n",
    "        #dense_edges = to_dense_adj(edge_index, batch)\n",
    "        #dense_edges = dense_edges.view(encoded.shape[0], encoded.shape[1], encoded.shape[1])\n",
    "\n",
    "        encoded = self.encoder(encoded)\n",
    "\n",
    "        token_predictions = self.token_prediction(encoded)\n",
    "        \n",
    "        return encoded, F.softmax(token_predictions)\n",
    "\n",
    "    def _step(self, batch, stage: str):\n",
    "\n",
    "        bb, raw, mask_id, original_token = batch\n",
    "\n",
    "        _, masked_token = self.forward(bb.x, bb.edge_index, bb.batch)\n",
    "\n",
    "        dense_x, _ = to_dense_batch(bb.x, bb.batch)\n",
    "\n",
    "        log_prefix = \"train\" if stage == 'train' else \"val\"\n",
    "        target_token = dense_x.clone()\n",
    "\n",
    "        for i in range(self.config.batch_size):\n",
    "            if mask_id[i] != 0:\n",
    "                target_token[i, mask_id[i]] = original_token[i]\n",
    "\n",
    "        loss = F.cross_entropy(masked_token.view(-1, self.config.num_opcodes), target_token.view(-1).long())\n",
    "\n",
    "        self.log(f\"{log_prefix}_loss\", loss, on_epoch=True, batch_size=self.config.batch_size)\n",
    "\n",
    "        return loss, bb, raw\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _, _ = self._step(batch, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, _, _ = self._step(batch, 'val')\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.config.learning_rate, weight_decay=1e-3)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.1)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:29:37.086066924Z",
     "start_time": "2023-11-19T12:29:37.042549998Z"
    }
   },
   "id": "bb3e0a658f2faf97"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:25:09.706046747Z",
     "start_time": "2023-11-19T12:24:48.269556099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 347988 samples\n"
     ]
    }
   ],
   "source": [
    "from llvm_ml.torch import BasicBlockDataset\n",
    "banned_ids = []\n",
    "dataset = BasicBlockDataset(\"./data/ryzen3600_v16.cbuf\", masked=False, banned_ids=banned_ids, prefilter=True)\n",
    "print(f\"Training with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                                | Type                            | Params\n",
      "-----------------------------------------------------------------------------------------\n",
      "0  | embedding                           | MCEmbedding                     | 10.8 M\n",
      "1  | embedding.embedding                 | Embedding                       | 5.4 M \n",
      "2  | embedding.gcn_embedding             | Embedding                       | 5.4 M \n",
      "3  | embedding.pos_encoding              | PositionalEncoding              | 0     \n",
      "4  | embedding.gcn                       | GCNConv                         | 65.8 K\n",
      "5  | embedding.gcn.aggr_module           | SumAggregation                  | 0     \n",
      "6  | embedding.gcn.lin                   | Linear                          | 65.5 K\n",
      "7  | embedding.norm                      | LayerNorm                       | 512   \n",
      "8  | encoder_layer                       | TransformerEncoderLayer         | 1.3 M \n",
      "9  | encoder_layer.self_attn             | MultiheadAttention              | 263 K \n",
      "10 | encoder_layer.self_attn.out_proj    | NonDynamicallyQuantizableLinear | 65.8 K\n",
      "11 | encoder_layer.linear1               | Linear                          | 526 K \n",
      "12 | encoder_layer.dropout               | Dropout                         | 0     \n",
      "13 | encoder_layer.linear2               | Linear                          | 524 K \n",
      "14 | encoder_layer.norm1                 | LayerNorm                       | 512   \n",
      "15 | encoder_layer.norm2                 | LayerNorm                       | 512   \n",
      "16 | encoder_layer.dropout1              | Dropout                         | 0     \n",
      "17 | encoder_layer.dropout2              | Dropout                         | 0     \n",
      "18 | encoder                             | TransformerEncoder              | 5.3 M \n",
      "19 | encoder.layers                      | ModuleList                      | 5.3 M \n",
      "20 | encoder.layers.0                    | TransformerEncoderLayer         | 1.3 M \n",
      "21 | encoder.layers.0.self_attn          | MultiheadAttention              | 263 K \n",
      "22 | encoder.layers.0.self_attn.out_proj | NonDynamicallyQuantizableLinear | 65.8 K\n",
      "23 | encoder.layers.0.linear1            | Linear                          | 526 K \n",
      "24 | encoder.layers.0.dropout            | Dropout                         | 0     \n",
      "25 | encoder.layers.0.linear2            | Linear                          | 524 K \n",
      "26 | encoder.layers.0.norm1              | LayerNorm                       | 512   \n",
      "27 | encoder.layers.0.norm2              | LayerNorm                       | 512   \n",
      "28 | encoder.layers.0.dropout1           | Dropout                         | 0     \n",
      "29 | encoder.layers.0.dropout2           | Dropout                         | 0     \n",
      "30 | encoder.layers.1                    | TransformerEncoderLayer         | 1.3 M \n",
      "31 | encoder.layers.1.self_attn          | MultiheadAttention              | 263 K \n",
      "32 | encoder.layers.1.self_attn.out_proj | NonDynamicallyQuantizableLinear | 65.8 K\n",
      "33 | encoder.layers.1.linear1            | Linear                          | 526 K \n",
      "34 | encoder.layers.1.dropout            | Dropout                         | 0     \n",
      "35 | encoder.layers.1.linear2            | Linear                          | 524 K \n",
      "36 | encoder.layers.1.norm1              | LayerNorm                       | 512   \n",
      "37 | encoder.layers.1.norm2              | LayerNorm                       | 512   \n",
      "38 | encoder.layers.1.dropout1           | Dropout                         | 0     \n",
      "39 | encoder.layers.1.dropout2           | Dropout                         | 0     \n",
      "40 | encoder.layers.2                    | TransformerEncoderLayer         | 1.3 M \n",
      "41 | encoder.layers.2.self_attn          | MultiheadAttention              | 263 K \n",
      "42 | encoder.layers.2.self_attn.out_proj | NonDynamicallyQuantizableLinear | 65.8 K\n",
      "43 | encoder.layers.2.linear1            | Linear                          | 526 K \n",
      "44 | encoder.layers.2.dropout            | Dropout                         | 0     \n",
      "45 | encoder.layers.2.linear2            | Linear                          | 524 K \n",
      "46 | encoder.layers.2.norm1              | LayerNorm                       | 512   \n",
      "47 | encoder.layers.2.norm2              | LayerNorm                       | 512   \n",
      "48 | encoder.layers.2.dropout1           | Dropout                         | 0     \n",
      "49 | encoder.layers.2.dropout2           | Dropout                         | 0     \n",
      "50 | encoder.layers.3                    | TransformerEncoderLayer         | 1.3 M \n",
      "51 | encoder.layers.3.self_attn          | MultiheadAttention              | 263 K \n",
      "52 | encoder.layers.3.self_attn.out_proj | NonDynamicallyQuantizableLinear | 65.8 K\n",
      "53 | encoder.layers.3.linear1            | Linear                          | 526 K \n",
      "54 | encoder.layers.3.dropout            | Dropout                         | 0     \n",
      "55 | encoder.layers.3.linear2            | Linear                          | 524 K \n",
      "56 | encoder.layers.3.norm1              | LayerNorm                       | 512   \n",
      "57 | encoder.layers.3.norm2              | LayerNorm                       | 512   \n",
      "58 | encoder.layers.3.dropout1           | Dropout                         | 0     \n",
      "59 | encoder.layers.3.dropout2           | Dropout                         | 0     \n",
      "60 | token_prediction                    | Linear                          | 5.4 M \n",
      "-----------------------------------------------------------------------------------------\n",
      "22.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.8 M    Total params\n",
      "91.169    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cde82a04a71646aca1609234b7349a04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105500/3430780934.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return encoded, F.softmax(token_predictions)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11534efd3288474cb55077e12a9223e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e87cf23cd5e2416ab53394f68c75865c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c4395d30614c93a3fed5a6f9f7d015"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97414b16983f4a1ea54917a91306f44b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fced7882902d45aca10008ccc3a13b24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a3973a3b29849b5a9de6f160269876d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d2297733653477cb07d0b3ce19652ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1685cd871def4d78b37a761db73d219d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b436713e5a064cdcb46a5e5e01c941f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3c532f1e831458e9df5a4b5e9c966a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "486cfdbe464c4e40b41d17c264544e94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85907277be694a8c8684ac8b5424ac1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "306023b593674e59a436fea776ef0e37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd708b74c9c948b4b5fbaf2967b79142"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43d8cb4642a54f888d3d6698f7513942"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4036f52991a9475e94c9ea5028aa5dd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08bf573440fd4d64b4254d48edfda529"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9587458f4b74a429252268fb292754e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8f29197085545829b7c780f9bafed65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ded77c34c556447bbcdf736a3745f469"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fc7fd97a77544f295460d3306f608bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "494eb203add54da18284e307536f0a1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e55fd104b0345dd9714525ffff842d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af9728c2a38f4ed9aa10b3aba20d37dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c96a81f0652b41ce89264d2b4b837d16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ee6bbd6789a431d86db2cd2febb34a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "#from model.GraphBERT import ThroughputEstimator\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelSummary, LearningRateMonitor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
    "\n",
    "config = MCNNConfig(dataset.num_opcodes)\n",
    "config.learning_rate = 1e-4\n",
    "config.batch_size = 256\n",
    "config.hidden_size = 128\n",
    "config.embedding_size = 256\n",
    "# config.reg_forward_expansion = 16\n",
    "config.forward_expansion = 2\n",
    "\n",
    "num_training = int(0.7 * len(dataset))\n",
    "num_val = len(dataset) - num_training\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [num_training, num_val])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "\n",
    "model = MCBERT(config)\n",
    "\n",
    "logger = TensorBoardLogger(\"runs\", name=\"bert\")\n",
    "logger.log_graph(model)\n",
    "callbacks = [\n",
    "    ModelSummary(max_depth=-1),\n",
    "    LearningRateMonitor(),\n",
    "]\n",
    "trainer = pl.Trainer(max_epochs=25,\n",
    "                     logger=logger,\n",
    "                     precision='16-mixed',\n",
    "                     callbacks=callbacks,\n",
    "                     # fast_dev_run=True,\n",
    "                     #overfit_batches=1,\n",
    "                     #log_every_n_steps=1,\n",
    "                     )\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T12:58:44.392856853Z",
     "start_time": "2023-11-19T12:29:39.288419013Z"
    }
   },
   "id": "3ed1dc75f632603"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce507258e5bce8d7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "torch.save(model, \"data/bert_ryzen3600.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T10:34:24.757338053Z",
     "start_time": "2023-09-30T10:34:24.715909493Z"
    }
   },
   "id": "f1a90dfece6626c4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c059ecef510633c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save({'state_dict': model.state_dict()}, \"data/ryzen3600.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-30T09:08:26.249165838Z"
    }
   },
   "id": "8ddcfa81c7e3cfea"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "951e8d64644674f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
